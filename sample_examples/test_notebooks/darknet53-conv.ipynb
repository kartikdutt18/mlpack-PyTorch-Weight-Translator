{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596811509002",
   "display_name": "Python 3.7.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "def conv_batch(in_num, out_num, kernel_size=3, padding=1, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_num, out_num, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "        nn.BatchNorm2d(out_num),\n",
    "        nn.LeakyReLU())\n",
    "\n",
    "\n",
    "# Residual block\n",
    "class DarkResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(DarkResidualBlock, self).__init__()\n",
    "\n",
    "        reduced_channels = int(in_channels/2)\n",
    "        self.layer1 = conv_batch(in_channels, reduced_channels, kernel_size=1, padding=0)\n",
    "        self.layer2 = conv_batch(reduced_channels, in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "\n",
    "class Darknet53(nn.Module):\n",
    "    def __init__(self, block, num_classes):\n",
    "        super(Darknet53, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = conv_batch(3, 32)\n",
    "        self.conv2 = conv_batch(32, 64, stride=2)\n",
    "        self.residual_block1 = self.make_layer(block, in_channels=64, num_blocks=1)\n",
    "        self.conv3 = conv_batch(64, 128, stride=2)\n",
    "        self.residual_block2 = self.make_layer(block, in_channels=128, num_blocks=2)\n",
    "        self.conv4 = conv_batch(128, 256, stride=2)\n",
    "        self.residual_block3 = self.make_layer(block, in_channels=256, num_blocks=8)\n",
    "        self.conv5 = conv_batch(256, 512, stride=2)\n",
    "        self.residual_block4 = self.make_layer(block, in_channels=512, num_blocks=8)\n",
    "        self.conv6 = conv_batch(512, 1024, stride=2)\n",
    "        self.residual_block5 = self.make_layer(block, in_channels=1024, num_blocks=4)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(1024, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.residual_block1(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.residual_block2(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.residual_block3(out)\n",
    "        out = self.conv5(out)\n",
    "        out = self.residual_block4(out)\n",
    "        out = self.conv6(out)\n",
    "        out = self.residual_block5(out)\n",
    "        out = self.global_avg_pool(out)\n",
    "        out = out.view(-1, 1024)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def represent_model(self) :\n",
    "        self.features = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.conv2,\n",
    "            self.residual_block1[0].layer1,\n",
    "            self.residual_block1[0].layer2,\n",
    "            self.conv3,\n",
    "            self.residual_block2[0].layer1,\n",
    "            self.residual_block2[0].layer2,\n",
    "            self.residual_block2[1].layer1,\n",
    "            self.residual_block2[1].layer2,\n",
    "            self.conv4,\n",
    "            self.residual_block3[0].layer1,\n",
    "            self.residual_block3[0].layer2,\n",
    "            self.residual_block3[1].layer1,\n",
    "            self.residual_block3[1].layer2,\n",
    "            self.residual_block3[2].layer1,\n",
    "            self.residual_block3[2].layer2,\n",
    "            self.residual_block3[3].layer1,\n",
    "            self.residual_block3[3].layer2,\n",
    "            self.residual_block3[4].layer1,\n",
    "            self.residual_block3[4].layer2,\n",
    "            self.residual_block3[5].layer1,\n",
    "            self.residual_block3[5].layer2,\n",
    "            self.residual_block3[6].layer1,\n",
    "            self.residual_block3[6].layer2,\n",
    "            self.residual_block3[7].layer1,\n",
    "            self.residual_block3[7].layer2,\n",
    "            self.conv5,\n",
    "            self.residual_block4[0].layer1,\n",
    "            self.residual_block4[0].layer2,\n",
    "            self.residual_block4[1].layer1,\n",
    "            self.residual_block4[1].layer2,\n",
    "            self.residual_block4[2].layer1,\n",
    "            self.residual_block4[2].layer2,\n",
    "            self.residual_block4[3].layer1,\n",
    "            self.residual_block4[3].layer2,\n",
    "            self.residual_block4[4].layer1,\n",
    "            self.residual_block4[4].layer2,\n",
    "            self.residual_block4[5].layer1,\n",
    "            self.residual_block4[5].layer2,\n",
    "            self.residual_block4[6].layer1,\n",
    "            self.residual_block4[6].layer2,\n",
    "            self.residual_block4[7].layer1,\n",
    "            self.residual_block4[7].layer2,\n",
    "            self.conv6,\n",
    "            self.residual_block5[0].layer1,\n",
    "            self.residual_block5[0].layer2,\n",
    "            self.residual_block5[1].layer1,\n",
    "            self.residual_block5[1].layer2,\n",
    "            self.residual_block5[2].layer1,\n",
    "            self.residual_block5[2].layer2,\n",
    "            self.residual_block5[3].layer1,\n",
    "            self.residual_block5[3].layer2)\n",
    "\n",
    "        self.classifier = nn.Sequential(nn.Sequential(self.global_avg_pool, self.fc))\n",
    "\n",
    "    def make_layer(self, block, in_channels, num_blocks):\n",
    "        layers = []\n",
    "        for i in range(0, num_blocks):\n",
    "            layers.append(block(in_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def darknet53(num_classes):\n",
    "        return Darknet53(DarkResidualBlock, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet53.darknet53(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_best.pth.tar\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.represent_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.FloatTensor(np.array([1.0] * 224 * 224 * 3))\n",
    "input_tensor = input_tensor.reshape([1, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "def get_output(layer, eval = True) :\n",
    "    if eval:\n",
    "        layer = layer.eval()\n",
    "    global input_tensor, idx\n",
    "    input_tensor = layer(input_tensor)\n",
    "    idx += 1\n",
    "    print(\"Output of layer \", idx, \" : \", input_tensor.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset():\n",
    "    global input_tensor, idx\n",
    "    input_tensor = torch.FloatTensor(np.array([1.0] * 224 * 224 * 3))\n",
    "    input_tensor = input_tensor.reshape([1, 3, 224, 224])\n",
    "    idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output of layer  1  :  tensor(-66494.6406, grad_fn=<SumBackward0>)\nOutput of layer  2  :  tensor(91958.9922, grad_fn=<SumBackward0>)\nOutput of layer  3  :  tensor(213123.9062, grad_fn=<SumBackward0>)\n"
    }
   ],
   "source": [
    "model = model.eval()\n",
    "reset()\n",
    "for layer in model.conv1:\n",
    "    get_output(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output of layer  4  :  tensor(-60583.6055, grad_fn=<SumBackward0>)\nOutput of layer  5  :  tensor(28021.8281, grad_fn=<SumBackward0>)\nOutput of layer  6  :  tensor(88063.1875, grad_fn=<SumBackward0>)\n"
    }
   ],
   "source": [
    "for layer in model.conv2:\n",
    "    get_output(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output of layer  7  :  tensor(178315.9844, grad_fn=<SumBackward0>)\n"
    }
   ],
   "source": [
    "for layer in model.residual_block1:\n",
    "    get_output(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output of layer  8  :  tensor(-27250.8750, grad_fn=<SumBackward0>)\nOutput of layer  9  :  tensor(6702.3643, grad_fn=<SumBackward0>)\nOutput of layer  10  :  tensor(14910.2334, grad_fn=<SumBackward0>)\n"
    }
   ],
   "source": [
    "for layer in model.conv3:\n",
    "    get_output(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output of layer  11  :  tensor(29782.5117, grad_fn=<SumBackward0>)\nOutput of layer  12  :  tensor(36002.1445, grad_fn=<SumBackward0>)\n"
    }
   ],
   "source": [
    "for layer in model.residual_block2:\n",
    "    get_output(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output of layer  13  :  tensor(-18968.7227, grad_fn=<SumBackward0>)\nOutput of layer  14  :  tensor(5787.4888, grad_fn=<SumBackward0>)\nOutput of layer  15  :  tensor(14683.9355, grad_fn=<SumBackward0>)\n"
    }
   ],
   "source": [
    "for layer in model.conv4:\n",
    "    get_output(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output of layer  16  :  tensor(16319.0957, grad_fn=<SumBackward0>)\nOutput of layer  17  :  tensor(18173.0469, grad_fn=<SumBackward0>)\nOutput of layer  18  :  tensor(20638.9766, grad_fn=<SumBackward0>)\nOutput of layer  19  :  tensor(22421.3496, grad_fn=<SumBackward0>)\nOutput of layer  20  :  tensor(24824.5801, grad_fn=<SumBackward0>)\nOutput of layer  21  :  tensor(28561.0898, grad_fn=<SumBackward0>)\nOutput of layer  22  :  tensor(30829.2363, grad_fn=<SumBackward0>)\nOutput of layer  23  :  tensor(36881.2695, grad_fn=<SumBackward0>)\n"
    }
   ],
   "source": [
    "for layer in model.residual_block3:\n",
    "    get_output(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output of layer  24  :  tensor(-21828.9863, grad_fn=<SumBackward0>)\nOutput of layer  25  :  tensor(-6754.0391, grad_fn=<SumBackward0>)\nOutput of layer  26  :  tensor(3080.4380, grad_fn=<SumBackward0>)\n"
    }
   ],
   "source": [
    "for layer in model.conv5:\n",
    "    get_output(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output of layer  27  :  tensor(3371.2676, grad_fn=<SumBackward0>)\nOutput of layer  28  :  tensor(4013.7974, grad_fn=<SumBackward0>)\nOutput of layer  29  :  tensor(4991.7456, grad_fn=<SumBackward0>)\nOutput of layer  30  :  tensor(6286.2559, grad_fn=<SumBackward0>)\nOutput of layer  31  :  tensor(7087.5371, grad_fn=<SumBackward0>)\nOutput of layer  32  :  tensor(9094.2822, grad_fn=<SumBackward0>)\nOutput of layer  33  :  tensor(11172.4922, grad_fn=<SumBackward0>)\nOutput of layer  34  :  tensor(15173.5615, grad_fn=<SumBackward0>)\n"
    }
   ],
   "source": [
    "for layer in model.residual_block4:\n",
    "    get_output(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output of layer  35  :  tensor(-5633.3867, grad_fn=<SumBackward0>)\nOutput of layer  36  :  tensor(-4110.5840, grad_fn=<SumBackward0>)\nOutput of layer  37  :  tensor(1388.2852, grad_fn=<SumBackward0>)\n"
    }
   ],
   "source": [
    "for layer in model.conv6:\n",
    "    get_output(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output of layer  38  :  tensor(1942.5583, grad_fn=<SumBackward0>)\nOutput of layer  39  :  tensor(2399.0015, grad_fn=<SumBackward0>)\nOutput of layer  40  :  tensor(3401.9531, grad_fn=<SumBackward0>)\nOutput of layer  41  :  tensor(7979.6392, grad_fn=<SumBackward0>)\n"
    }
   ],
   "source": [
    "for layer in model.residual_block5:\n",
    "    get_output(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = model.global_avg_pool(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(162.8498, grad_fn=<SumBackward0>)\n"
    }
   ],
   "source": [
    "print(input_tensor.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(62.7226, grad_fn=<SumBackward0>)"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "input_tensor[-1, 101:501].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 1024])"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "input_tensor.view(-1, 1024).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(6.4653, grad_fn=<SumBackward0>)"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "input_tensor = input_tensor.view(-1, 1024)\n",
    "model.fc(input_tensor).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(3.6737, grad_fn=<MaxBackward1>)"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "model.fc(input_tensor).max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(623)"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "model.fc(input_tensor).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(6.4653, grad_fn=<SumBackward0>)"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "reset()\n",
    "model.eval()\n",
    "model(input_tensor).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "n03394916\ntensor(601)  ----> tensor(8.7842, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00003759.jpg ------> tensor(68339.8203)\ntensor(566)  ----> tensor(13.2638, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00005548.jpg ------> tensor(25738.9199)\ntensor(566)  ----> tensor(15.0629, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00007985.jpg ------> tensor(46697.1094)\ntensor(513)  ----> tensor(24.3026, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00005554.jpg ------> tensor(19992.7109)\ntensor(566)  ----> tensor(17.3594, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00007155.jpg ------> tensor(25416.3613)\ntensor(566)  ----> tensor(12.3731, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00000957.jpg ------> tensor(25532.3027)\ntensor(513)  ----> tensor(10.9770, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00003587.jpg ------> tensor(68653.9297)\ntensor(566)  ----> tensor(11.1971, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00007536.jpg ------> tensor(34702.4531)\nn03417042\ntensor(569)  ----> tensor(17.7112, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00006234.jpg ------> tensor(70846.1562)\ntensor(569)  ----> tensor(12.5220, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00007477.jpg ------> tensor(56817.3164)\ntensor(569)  ----> tensor(12.3389, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00002434.jpg ------> tensor(86127.0391)\ntensor(569)  ----> tensor(13.2445, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00004119.jpg ------> tensor(67453.3125)\ntensor(569)  ----> tensor(14.3060, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00005744.jpg ------> tensor(40716.9531)\ntensor(569)  ----> tensor(12.3115, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00007519.jpg ------> tensor(97504.8203)\ntensor(569)  ----> tensor(14.8963, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00001144.jpg ------> tensor(80293.3594)\ntensor(569)  ----> tensor(10.9099, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00005094.jpg ------> tensor(72900.7188)\nn03445777\ntensor(574)  ----> tensor(10.0623, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00011983.jpg ------> tensor(20079.3555)\ntensor(605)  ----> tensor(10.8330, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00012356.jpg ------> tensor(90457.7188)\ntensor(574)  ----> tensor(10.8750, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00007947.jpg ------> tensor(60117.6250)\ntensor(529)  ----> tensor(9.1174, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00006468.jpg ------> tensor(97427.3516)\ntensor(481)  ----> tensor(8.8989, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00008599.jpg ------> tensor(46901.8984)\ntensor(574)  ----> tensor(14.8684, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00003793.jpg ------> tensor(95533.5078)\ntensor(574)  ----> tensor(12.0453, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00002314.jpg ------> tensor(56558.8320)\nn02102040\ntensor(217)  ----> tensor(11.3495, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00001968.jpg ------> tensor(49349.4062)\ntensor(217)  ----> tensor(13.5225, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00002294.jpg ------> tensor(48657.7344)\ntensor(217)  ----> tensor(14.8610, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00004553.jpg ------> tensor(81548.3203)\ntensor(217)  ----> tensor(16.3418, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00008334.jpg ------> tensor(64591.5469)\ntensor(217)  ----> tensor(15.0602, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00007568.jpg ------> tensor(52404.4570)\ntensor(217)  ----> tensor(17.7765, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00000665.jpg ------> tensor(65291.7422)\ntensor(196)  ----> tensor(12.4881, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00004548.jpg ------> tensor(58458.6836)\ntensor(217)  ----> tensor(12.4923, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00002315.jpg ------> tensor(72393.3672)\nn03425413\ntensor(571)  ----> tensor(9.1056, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00006434.jpg ------> tensor(31865.3535)\ntensor(899)  ----> tensor(9.0215, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00005996.jpg ------> tensor(78534.6172)\ntensor(778)  ----> tensor(12.9410, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00006909.jpg ------> tensor(73664.6094)\ntensor(571)  ----> tensor(11.4288, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00007186.jpg ------> tensor(75351.3984)\ntensor(571)  ----> tensor(9.8838, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00005183.jpg ------> tensor(70892.2500)\ntensor(794)  ----> tensor(11.0148, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00009279.jpg ------> tensor(55519.4180)\ntensor(778)  ----> tensor(13.7047, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00002049.jpg ------> tensor(65487.6875)\ntensor(480)  ----> tensor(12.2930, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00002498.jpg ------> tensor(51524.0938)\nn03888257\ntensor(701)  ----> tensor(11.2528, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00002508.jpg ------> tensor(93312.0469)\ntensor(701)  ----> tensor(16.0078, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00004745.jpg ------> tensor(105004.2031)\ntensor(701)  ----> tensor(20.5034, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00007616.jpg ------> tensor(83262.8516)\ntensor(701)  ----> tensor(13.0772, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00009559.jpg ------> tensor(114944.2656)\ntensor(452)  ----> tensor(9.3652, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00009593.jpg ------> tensor(74154.4297)\ntensor(701)  ----> tensor(13.3342, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00009873.jpg ------> tensor(84641.3125)\ntensor(701)  ----> tensor(10.2356, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00002665.jpg ------> tensor(91215.2031)\ntensor(701)  ----> tensor(10.9416, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00009269.jpg ------> tensor(68692.8281)\nn03028079\ntensor(483)  ----> tensor(10.9608, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00014037.jpg ------> tensor(32365.4375)\ntensor(442)  ----> tensor(9.8268, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00009705.jpg ------> tensor(33312.3008)\ntensor(803)  ----> tensor(8.4299, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00004939.jpg ------> tensor(57548.2383)\ntensor(442)  ----> tensor(9.8181, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00006268.jpg ------> tensor(55612.4141)\ntensor(442)  ----> tensor(10.2923, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00005598.jpg ------> tensor(82403.7031)\ntensor(442)  ----> tensor(12.9296, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00015155.jpg ------> tensor(69203.3203)\ntensor(663)  ----> tensor(12.8317, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00008116.jpg ------> tensor(48304.7266)\ntensor(668)  ----> tensor(9.5414, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00005295.jpg ------> tensor(52686.9141)\nn03000684\ntensor(843)  ----> tensor(11.6434, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00006578.jpg ------> tensor(62739.9336)\ntensor(748)  ----> tensor(13.4224, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00004034.jpg ------> tensor(58789.3711)\ntensor(491)  ----> tensor(19.3289, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00000537.jpg ------> tensor(33931.0156)\ntensor(491)  ----> tensor(14.2049, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00005506.jpg ------> tensor(106346.4531)\ntensor(491)  ----> tensor(9.9369, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00006726.jpg ------> tensor(72118.3125)\ntensor(830)  ----> tensor(9.6176, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00009206.jpg ------> tensor(71358.9375)\ntensor(880)  ----> tensor(9.6628, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00006043.jpg ------> tensor(70351.6328)\ntensor(491)  ----> tensor(17.0088, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00006669.jpg ------> tensor(117735.0859)\nn01440764\ntensor(693)  ----> tensor(11.0457, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00003014.jpg ------> tensor(60614.0352)\ntensor(669)  ----> tensor(12.7682, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00000293.jpg ------> tensor(75931.4375)\ntensor(0)  ----> tensor(10.4130, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00006697.jpg ------> tensor(54051.1250)\ntensor(0)  ----> tensor(11.9105, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00009346.jpg ------> tensor(63854.3828)\ntensor(431)  ----> tensor(11.2635, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00007197.jpg ------> tensor(64716.4180)\ntensor(0)  ----> tensor(12.5723, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00009379.jpg ------> tensor(59967.7266)\ntensor(0)  ----> tensor(11.1166, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00009396.jpg ------> tensor(95792.0156)\ntensor(395)  ----> tensor(10.5357, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00002138.jpg ------> tensor(51615.7656)\nn02979186\ntensor(534)  ----> tensor(8.3184, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00003944.jpg ------> tensor(60155.7266)\ntensor(754)  ----> tensor(13.2726, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00009833.jpg ------> tensor(89986.2500)\ntensor(481)  ----> tensor(10.8622, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00000557.jpg ------> tensor(60346.7969)\ntensor(848)  ----> tensor(15.0666, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00009404.jpg ------> tensor(107644.9766)\ntensor(848)  ----> tensor(12.4213, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00002034.jpg ------> tensor(80150.)\ntensor(713)  ----> tensor(13.3017, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00007226.jpg ------> tensor(96157.5391)\ntensor(818)  ----> tensor(11.6741, grad_fn=<SelectBackward>)  --->  ILSVRC2012_val_00005866.jpg ------> tensor(122680.0391)\n"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image \n",
    "model.eval()\n",
    "classes = os.listdir(\"../imagenette-test\")\n",
    "for folder in classes :\n",
    "    print(folder)\n",
    "    images = os.listdir(\"../imagenette-test/\" + folder)\n",
    "    for image in images:\n",
    "        train_transforms = transforms.Compose([transforms.Resize(224), transforms.ToTensor()])\n",
    "        img = Image.open(\"../imagenette-test/\" + folder + \"/\" + image)\n",
    "        img = train_transforms(img)\n",
    "        temp = img\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "        index = torch.argmax(model(img))\n",
    "        print(index, \" ---->\", model(img)[0][index], \" ---> \", image, \"------>\" ,img.sum())\n",
    "        mat.append(img.numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "def make_directory(base_path : str) -> int :\n",
    "    \"\"\"\n",
    "        Checks if a directory exists and if doesn't creates the directory.\n",
    "\n",
    "        Args:\n",
    "        base_path : Directory path which will be created if it doesn't exist.\n",
    "\n",
    "        Returns 0 if directory exists else 1\n",
    "    \"\"\"\n",
    "    if os.path.exists(base_path) :\n",
    "        return 0\n",
    "\n",
    "    # Create the directory since the path doesn't exist.\n",
    "    os.mkdir(base_path)\n",
    "    if os.path.exists(base_path) :\n",
    "        return 0\n",
    "\n",
    "    # Path doesn't exist as well as directory couldn't be created.\n",
    "    print(\"Error : Cannot create desired path : \", base_path)\n",
    "    return 1\n",
    "\n",
    "def generate_csv(csv_name : str, weight_matrix : torch.tensor, base_path : str) -> str :\n",
    "    \"\"\"\n",
    "        Generates csv for weights or bias matrix.\n",
    "\n",
    "        Args:\n",
    "        csv_name : A string name for csv file which will store the weights.\n",
    "        weight_matrix : A torch tensor holding weights that will be stored in the matrix.\n",
    "        base_path : Base path where csv will be stored.\n",
    "    \"\"\"\n",
    "    # Check if base path exists else create directory.\n",
    "    make_directory(base_path)\n",
    "    file_path = os.path.join(base_path, csv_name)\n",
    "    matrix = weight_matrix.numpy().ravel()\n",
    "    np.savetxt(file_path, matrix, fmt='%1.128f')\n",
    "    return file_path\n",
    "\n",
    "def extract_weights(layer, layer_index, base_path) -> {} :\n",
    "    \"\"\"\n",
    "        Extracts weights, biases and other parameters required to reproduce\n",
    "        the same output.\n",
    "\n",
    "        Args:\n",
    "        layer : An torch.nn object (layer).\n",
    "        layer_index : A string determining name of csv file that will be appended to\n",
    "                      name of layer.\n",
    "                      Eg. if layer = nn.Conv2d and layer_index = 0\n",
    "                          csv_filename = Conv_layer_index.csv\n",
    "        base_path : A string depicting base path for storing weight / bias csv.\n",
    "\n",
    "        Returns dictionary of parameter description and parameters.\n",
    "\n",
    "        Exceptions:\n",
    "        Currently this has only been tested for convolutional and batch-norm layer.\n",
    "    \"\"\"\n",
    "    parameter_dictionary = {}\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        # The layer corresponds to Convolutional layer.\n",
    "        # For convolution layer we require weights and biases to reproduce the\n",
    "        # same result.\n",
    "        parameter_dictionary[\"name\"] = \"Convolution2D\"\n",
    "        parameter_dictionary[\"input-channels\"] = layer.in_channels\n",
    "        parameter_dictionary[\"output-channels\"] = layer.out_channels\n",
    "        # Assume weight matrix is never empty for nn.Conv2d()\n",
    "        parameter_dictionary[\"has_weights\"] = 1\n",
    "        parameter_dictionary[\"weight_offset\"] = 0\n",
    "        csv_name = \"conv_weight_\" + layer_index + \".csv\"\n",
    "        parameter_dictionary[\"weight_csv\"] = generate_csv(csv_name, \\\n",
    "            layer.weight.detach(), base_path)\n",
    "        if layer.bias != None:\n",
    "            parameter_dictionary[\"has_bias\"] = 1\n",
    "            parameter_dictionary[\"bias_offset\"] = 0\n",
    "            bias_csv_name = \"conv_bias_\" + layer_index + \".csv\"\n",
    "            parameter_dictionary[\"bias_csv\"] = generate_csv(bias_csv_name, \\\n",
    "                layer.bias.detach(), base_path)\n",
    "        else:\n",
    "            parameter_dictionary[\"has_bias\"] = 0\n",
    "            parameter_dictionary[\"bias_offset\"] = layer.out_channels\n",
    "            parameter_dictionary[\"bias_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_mean\"] = 0\n",
    "        parameter_dictionary[\"running_mean_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_var\"] = 0\n",
    "        parameter_dictionary[\"running_var_csv\"] = \"None\"\n",
    "    elif isinstance(layer, nn.BatchNorm2d) :\n",
    "        # The layer corresponds to Batch Normalization layer.\n",
    "        # For batchnorm layer we require weights, biases and running mean and running variance\n",
    "        # to reproduce the same result.\n",
    "        parameter_dictionary[\"name\"] = \"BatchNorm2D\"\n",
    "        parameter_dictionary[\"input-channels\"] = layer.num_features\n",
    "        parameter_dictionary[\"output-channels\"] = layer.num_features\n",
    "        # Assume weight matrix is never empty for nn.BatchNorm2d()\n",
    "        parameter_dictionary[\"has_weights\"] = 1\n",
    "        parameter_dictionary[\"weight_offset\"] = 0\n",
    "        csv_name = \"batchnorm_weight_\" + layer_index + \".csv\"\n",
    "        parameter_dictionary[\"weight_csv\"] = generate_csv(csv_name, \\\n",
    "            layer.weight.detach(), base_path)\n",
    "        if layer.bias != None:\n",
    "            parameter_dictionary[\"has_bias\"] = 1\n",
    "            parameter_dictionary[\"bias_offset\"] = 0\n",
    "            bias_csv_name = \"batchnorm_bias_\" + layer_index + \".csv\"\n",
    "            parameter_dictionary[\"bias_csv\"] = generate_csv(bias_csv_name, \\\n",
    "                layer.bias.detach(), base_path)\n",
    "        else:\n",
    "            parameter_dictionary[\"has_bias\"] = 0\n",
    "            parameter_dictionary[\"bias_offset\"] = layer.out_channels\n",
    "            parameter_dictionary[\"bias_csv\"] = \"None\"\n",
    "        # Assume BatchNorm layer always running variance and running mean.\n",
    "        running_mean_csv = \"batchnorm_running_mean_\" + layer_index + \".csv\"\n",
    "        parameter_dictionary[\"has_running_mean\"] = 1\n",
    "        parameter_dictionary[\"running_mean_csv\"] = generate_csv(running_mean_csv, \\\n",
    "            layer.running_mean.detach(), base_path)\n",
    "        parameter_dictionary[\"has_running_var\"] = 1\n",
    "        running_var_csv = \"batchnorm_running_var_\" + layer_index + \".csv\" \n",
    "        parameter_dictionary[\"running_var_csv\"] = generate_csv(running_var_csv, \\\n",
    "            layer.running_var.detach(), base_path)\n",
    "    elif (isinstance(layer, nn.Linear)) :\n",
    "        # The layer corresponds to Convolutional layer.\n",
    "        # For convolution layer we require weights and biases to reproduce the\n",
    "        # same result.\n",
    "        parameter_dictionary[\"name\"] = \"Linear\"\n",
    "        parameter_dictionary[\"input-channels\"] = layer.in_features\n",
    "        parameter_dictionary[\"output-channels\"] = layer.out_features\n",
    "        # Assume weight matrix is never empty for nn.Linear()\n",
    "        parameter_dictionary[\"has_weights\"] = 1\n",
    "        parameter_dictionary[\"weight_offset\"] = 0\n",
    "        csv_name = \"linear_weight_\" + layer_index + \".csv\"\n",
    "        parameter_dictionary[\"weight_csv\"] = generate_csv(csv_name, \\\n",
    "            layer.weight.detach(), base_path)\n",
    "        if layer.bias != None:\n",
    "            parameter_dictionary[\"has_bias\"] = 1\n",
    "            parameter_dictionary[\"bias_offset\"] = 0\n",
    "            bias_csv_name = \"linear_bias_\" + layer_index + \".csv\"\n",
    "            parameter_dictionary[\"bias_csv\"] = generate_csv(bias_csv_name, \\\n",
    "                layer.bias.detach(), base_path)\n",
    "        else:\n",
    "            parameter_dictionary[\"has_bias\"] = 0\n",
    "            parameter_dictionary[\"bias_offset\"] = layer.out_features\n",
    "            parameter_dictionary[\"bias_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_mean\"] = 0\n",
    "        parameter_dictionary[\"running_mean_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_var\"] = 0\n",
    "        parameter_dictionary[\"running_var_csv\"] = \"None\"\n",
    "    else :\n",
    "        # The layer corresponds to un-supported layer or layer doesn't have trainable\n",
    "        # parameter. Example of such layers are nn.MaxPooling2d() and nn.SoftMax.\n",
    "        parameter_dictionary[\"name\"] = \"unknown_layer\"\n",
    "        parameter_dictionary[\"input-channels\"] = 0\n",
    "        parameter_dictionary[\"output-channels\"] = 0\n",
    "        parameter_dictionary[\"has_weights\"] = 0\n",
    "        parameter_dictionary[\"weight_offset\"] = 0\n",
    "        parameter_dictionary[\"weight_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_bias\"] = 0\n",
    "        parameter_dictionary[\"bias_offset\"] = 0\n",
    "        parameter_dictionary[\"bias_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_mean\"] = 0\n",
    "        parameter_dictionary[\"running_mean_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_var\"] = 0\n",
    "        parameter_dictionary[\"running_var_csv\"] = \"None\"\n",
    "    return parameter_dictionary\n",
    "\n",
    "def create_xml_tree(parameter_dictionary : dict, root_tag = \"layer\") -> ElementTree.ElementTree() :\n",
    "    \"\"\"\n",
    "        Creates an XML tree from a dictionary wrapped around root tag.\n",
    "\n",
    "        Args:\n",
    "        parameter_dictionary : Dictionary which will be converted to xml tree.\n",
    "        root_tag : Tag around which elements of dictionary will be wrapped.\n",
    "                    Defaults to \"layer\".\n",
    "    \n",
    "        Returns : ElementTree.ElementTree() object.\n",
    "    \"\"\"\n",
    "    layer = ElementTree.Element(root_tag)\n",
    "    for parameter_desc in parameter_dictionary :\n",
    "        parameter_description = ElementTree.Element(parameter_desc)\n",
    "        parameter_description.text = str(parameter_dictionary[parameter_desc])\n",
    "        layer.append(parameter_description)\n",
    "    return layer\n",
    "\n",
    "def create_xml_file(parameter_dictionary : dict,\n",
    "                    xml_path : str,\n",
    "                    root_tag : str,\n",
    "                    element_tag : str) -> int :\n",
    "    \"\"\"\n",
    "        Appends layer description to xml file and if xml doesn't exist or is empty, \n",
    "        creates an xml file with required headers.\n",
    "\n",
    "        Args:\n",
    "        parameter_dictionary : Dictionary containing layer description.\n",
    "        xml_path : Path where xml file will be stored / created.\n",
    "        root_tag : Tag around which xml file will be wrapped.\n",
    "        element_tag : Tag around which each element in dictionary will be wrapped.\n",
    "    \"\"\"\n",
    "   \n",
    "    if not os.path.exists(xml_path) :\n",
    "        # Create base xml file.\n",
    "        f = open(xml_path, \"w\")\n",
    "        data = \"<\" + root_tag + \">\" + \"</\" + root_tag + \">\"\n",
    "        f.write(data)\n",
    "        f.close()\n",
    "    layer_description = create_xml_tree(parameter_dictionary, element_tag)\n",
    "    xml_file = ElementTree.parse(xml_path)\n",
    "    root = xml_file.getroot()\n",
    "    layer = root.makeelement(element_tag, parameter_dictionary)\n",
    "    root.append(layer_description)\n",
    "    xml_file.write(xml_path, encoding = \"unicode\")\n",
    "    return 0\n",
    "\n",
    "def iterate_over_layers(modules, xml_path, base_path, layer_index, debug : bool) -> int :\n",
    "    \"\"\"\n",
    "        Parses model and generates csv and xml file which will be iterated by C++ translator.\n",
    "    \n",
    "        Args:\n",
    "        modules : PyTorch model for which parameter csv and xml will be created.\n",
    "        xml_path : Directory where xml with model config will be saved.\n",
    "        base_path : Directory where csv will be stored.\n",
    "\n",
    "        Returns 0 if weights are created else return 1.\n",
    "    \"\"\"\n",
    "    for block in modules :\n",
    "        for layer in block :\n",
    "            layer_index += 1\n",
    "            parameter_dict = extract_weights(layer, str(layer_index), base_path)\n",
    "            create_xml_file(parameter_dict, xml_path, \"model\", \"layer\")\n",
    "            if not os.path.exists(parameter_dict[\"weight_csv\"]) and parameter_dict[\"has_weights\"] == 1:\n",
    "                print(\"Creating weights failed!\")\n",
    "                return 1, layer_index\n",
    "            if debug :\n",
    "                print(\"Weights created succesfully for \", parameter_dict[\"name\"], \" layer index :\", layer_index)\n",
    "    return 0, layer_index\n",
    "\n",
    "def parse_model(model, xml_path, base_path, debug : bool) -> int :\n",
    "    \"\"\"\n",
    "        Parses model and generates csv and xml file which will be iterated by C++ translator.\n",
    "    \n",
    "        Args:\n",
    "        model : PyTorch model for which parameter csv and xml will be created.\n",
    "        xml_path : Directory where xml with model config will be saved.\n",
    "        base_path : Directory where csv will be stored.\n",
    "\n",
    "        Returns 0 if weights are created else return 1.\n",
    "    \"\"\"\n",
    "    layer_index = 0\n",
    "    error, layer_index = iterate_over_layers(model.features, xml_path, base_path, layer_index, debug)\n",
    "    if error :\n",
    "        print(\"An error occured!\")\n",
    "        return 1\n",
    "    print(layer_index)\n",
    "    error, layer_index = iterate_over_layers(model.classifier, xml_path, base_path, layer_index, debug)\n",
    "    if error :\n",
    "        print(\"An error occured!\")\n",
    "        return 1\n",
    "    print(layer_index)\n",
    "    if debug :\n",
    "        print(\"Model weights saved! Happy mlpack-translation.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "model.represent_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Weights created succesfully for  Convolution2D  layer index : 1\nWeights created succesfully for  BatchNorm2D  layer index : 2\nWeights created succesfully for  unknown_layer  layer index : 3\nWeights created succesfully for  Convolution2D  layer index : 4\nWeights created succesfully for  BatchNorm2D  layer index : 5\nWeights created succesfully for  unknown_layer  layer index : 6\nWeights created succesfully for  Convolution2D  layer index : 7\nWeights created succesfully for  BatchNorm2D  layer index : 8\nWeights created succesfully for  unknown_layer  layer index : 9\nWeights created succesfully for  Convolution2D  layer index : 10\nWeights created succesfully for  BatchNorm2D  layer index : 11\nWeights created succesfully for  unknown_layer  layer index : 12\nWeights created succesfully for  Convolution2D  layer index : 13\nWeights created succesfully for  BatchNorm2D  layer index : 14\nWeights created succesfully for  unknown_layer  layer index : 15\nWeights created succesfully for  Convolution2D  layer index : 16\nWeights created succesfully for  BatchNorm2D  layer index : 17\nWeights created succesfully for  unknown_layer  layer index : 18\nWeights created succesfully for  Convolution2D  layer index : 19\nWeights created succesfully for  BatchNorm2D  layer index : 20\nWeights created succesfully for  unknown_layer  layer index : 21\nWeights created succesfully for  Convolution2D  layer index : 22\nWeights created succesfully for  BatchNorm2D  layer index : 23\nWeights created succesfully for  unknown_layer  layer index : 24\nWeights created succesfully for  Convolution2D  layer index : 25\nWeights created succesfully for  BatchNorm2D  layer index : 26\nWeights created succesfully for  unknown_layer  layer index : 27\nWeights created succesfully for  Convolution2D  layer index : 28\nWeights created succesfully for  BatchNorm2D  layer index : 29\nWeights created succesfully for  unknown_layer  layer index : 30\nWeights created succesfully for  Convolution2D  layer index : 31\nWeights created succesfully for  BatchNorm2D  layer index : 32\nWeights created succesfully for  unknown_layer  layer index : 33\nWeights created succesfully for  Convolution2D  layer index : 34\nWeights created succesfully for  BatchNorm2D  layer index : 35\nWeights created succesfully for  unknown_layer  layer index : 36\nWeights created succesfully for  Convolution2D  layer index : 37\nWeights created succesfully for  BatchNorm2D  layer index : 38\nWeights created succesfully for  unknown_layer  layer index : 39\nWeights created succesfully for  Convolution2D  layer index : 40\nWeights created succesfully for  BatchNorm2D  layer index : 41\nWeights created succesfully for  unknown_layer  layer index : 42\nWeights created succesfully for  Convolution2D  layer index : 43\nWeights created succesfully for  BatchNorm2D  layer index : 44\nWeights created succesfully for  unknown_layer  layer index : 45\nWeights created succesfully for  Convolution2D  layer index : 46\nWeights created succesfully for  BatchNorm2D  layer index : 47\nWeights created succesfully for  unknown_layer  layer index : 48\nWeights created succesfully for  Convolution2D  layer index : 49\nWeights created succesfully for  BatchNorm2D  layer index : 50\nWeights created succesfully for  unknown_layer  layer index : 51\nWeights created succesfully for  Convolution2D  layer index : 52\nWeights created succesfully for  BatchNorm2D  layer index : 53\nWeights created succesfully for  unknown_layer  layer index : 54\nWeights created succesfully for  Convolution2D  layer index : 55\nWeights created succesfully for  BatchNorm2D  layer index : 56\nWeights created succesfully for  unknown_layer  layer index : 57\nWeights created succesfully for  Convolution2D  layer index : 58\nWeights created succesfully for  BatchNorm2D  layer index : 59\nWeights created succesfully for  unknown_layer  layer index : 60\nWeights created succesfully for  Convolution2D  layer index : 61\nWeights created succesfully for  BatchNorm2D  layer index : 62\nWeights created succesfully for  unknown_layer  layer index : 63\nWeights created succesfully for  Convolution2D  layer index : 64\nWeights created succesfully for  BatchNorm2D  layer index : 65\nWeights created succesfully for  unknown_layer  layer index : 66\nWeights created succesfully for  Convolution2D  layer index : 67\nWeights created succesfully for  BatchNorm2D  layer index : 68\nWeights created succesfully for  unknown_layer  layer index : 69\nWeights created succesfully for  Convolution2D  layer index : 70\nWeights created succesfully for  BatchNorm2D  layer index : 71\nWeights created succesfully for  unknown_layer  layer index : 72\nWeights created succesfully for  Convolution2D  layer index : 73\nWeights created succesfully for  BatchNorm2D  layer index : 74\nWeights created succesfully for  unknown_layer  layer index : 75\nWeights created succesfully for  Convolution2D  layer index : 76\nWeights created succesfully for  BatchNorm2D  layer index : 77\nWeights created succesfully for  unknown_layer  layer index : 78\nWeights created succesfully for  Convolution2D  layer index : 79\nWeights created succesfully for  BatchNorm2D  layer index : 80\nWeights created succesfully for  unknown_layer  layer index : 81\nWeights created succesfully for  Convolution2D  layer index : 82\nWeights created succesfully for  BatchNorm2D  layer index : 83\nWeights created succesfully for  unknown_layer  layer index : 84\nWeights created succesfully for  Convolution2D  layer index : 85\nWeights created succesfully for  BatchNorm2D  layer index : 86\nWeights created succesfully for  unknown_layer  layer index : 87\nWeights created succesfully for  Convolution2D  layer index : 88\nWeights created succesfully for  BatchNorm2D  layer index : 89\nWeights created succesfully for  unknown_layer  layer index : 90\nWeights created succesfully for  Convolution2D  layer index : 91\nWeights created succesfully for  BatchNorm2D  layer index : 92\nWeights created succesfully for  unknown_layer  layer index : 93\nWeights created succesfully for  Convolution2D  layer index : 94\nWeights created succesfully for  BatchNorm2D  layer index : 95\nWeights created succesfully for  unknown_layer  layer index : 96\nWeights created succesfully for  Convolution2D  layer index : 97\nWeights created succesfully for  BatchNorm2D  layer index : 98\nWeights created succesfully for  unknown_layer  layer index : 99\nWeights created succesfully for  Convolution2D  layer index : 100\nWeights created succesfully for  BatchNorm2D  layer index : 101\nWeights created succesfully for  unknown_layer  layer index : 102\nWeights created succesfully for  Convolution2D  layer index : 103\nWeights created succesfully for  BatchNorm2D  layer index : 104\nWeights created succesfully for  unknown_layer  layer index : 105\nWeights created succesfully for  Convolution2D  layer index : 106\nWeights created succesfully for  BatchNorm2D  layer index : 107\nWeights created succesfully for  unknown_layer  layer index : 108\nWeights created succesfully for  Convolution2D  layer index : 109\nWeights created succesfully for  BatchNorm2D  layer index : 110\nWeights created succesfully for  unknown_layer  layer index : 111\nWeights created succesfully for  Convolution2D  layer index : 112\nWeights created succesfully for  BatchNorm2D  layer index : 113\nWeights created succesfully for  unknown_layer  layer index : 114\nWeights created succesfully for  Convolution2D  layer index : 115\nWeights created succesfully for  BatchNorm2D  layer index : 116\nWeights created succesfully for  unknown_layer  layer index : 117\nWeights created succesfully for  Convolution2D  layer index : 118\nWeights created succesfully for  BatchNorm2D  layer index : 119\nWeights created succesfully for  unknown_layer  layer index : 120\nWeights created succesfully for  Convolution2D  layer index : 121\nWeights created succesfully for  BatchNorm2D  layer index : 122\nWeights created succesfully for  unknown_layer  layer index : 123\nWeights created succesfully for  Convolution2D  layer index : 124\nWeights created succesfully for  BatchNorm2D  layer index : 125\nWeights created succesfully for  unknown_layer  layer index : 126\nWeights created succesfully for  Convolution2D  layer index : 127\nWeights created succesfully for  BatchNorm2D  layer index : 128\nWeights created succesfully for  unknown_layer  layer index : 129\nWeights created succesfully for  Convolution2D  layer index : 130\nWeights created succesfully for  BatchNorm2D  layer index : 131\nWeights created succesfully for  unknown_layer  layer index : 132\nWeights created succesfully for  Convolution2D  layer index : 133\nWeights created succesfully for  BatchNorm2D  layer index : 134\nWeights created succesfully for  unknown_layer  layer index : 135\nWeights created succesfully for  Convolution2D  layer index : 136\nWeights created succesfully for  BatchNorm2D  layer index : 137\nWeights created succesfully for  unknown_layer  layer index : 138\nWeights created succesfully for  Convolution2D  layer index : 139\nWeights created succesfully for  BatchNorm2D  layer index : 140\nWeights created succesfully for  unknown_layer  layer index : 141\nWeights created succesfully for  Convolution2D  layer index : 142\nWeights created succesfully for  BatchNorm2D  layer index : 143\nWeights created succesfully for  unknown_layer  layer index : 144\nWeights created succesfully for  Convolution2D  layer index : 145\nWeights created succesfully for  BatchNorm2D  layer index : 146\nWeights created succesfully for  unknown_layer  layer index : 147\nWeights created succesfully for  Convolution2D  layer index : 148\nWeights created succesfully for  BatchNorm2D  layer index : 149\nWeights created succesfully for  unknown_layer  layer index : 150\nWeights created succesfully for  Convolution2D  layer index : 151\nWeights created succesfully for  BatchNorm2D  layer index : 152\nWeights created succesfully for  unknown_layer  layer index : 153\nWeights created succesfully for  Convolution2D  layer index : 154\nWeights created succesfully for  BatchNorm2D  layer index : 155\nWeights created succesfully for  unknown_layer  layer index : 156\n156\nWeights created succesfully for  unknown_layer  layer index : 157\nWeights created succesfully for  Linear  layer index : 158\n158\nModel weights saved! Happy mlpack-translation.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "parse_model(model, \"./cfg/\" + \"darknet53\" + \".xml\", \"./models/\" + \"darknet53\" + \"/mlpack-weights/\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "41609928"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "get_n_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_batch(in_num, out_num, kernel_size=3, padding=1, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_num, out_num, kernel_size=kernel_size, stride=stride, padding=padding, bias=True),\n",
    "        nn.BatchNorm2d(out_num),\n",
    "        nn.LeakyReLU())\n",
    "model2 = Darknet53.darknet53(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "41627784"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "get_n_params(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: torchsummary in /Users/aarongreenburg/miniconda3/lib/python3.7/site-packages (1.5.1)\n"
    }
   ],
   "source": [
    "!python3 -m pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 32, 224, 224]             896\n       BatchNorm2d-2         [-1, 32, 224, 224]              64\n         LeakyReLU-3         [-1, 32, 224, 224]               0\n            Conv2d-4         [-1, 64, 112, 112]          18,496\n       BatchNorm2d-5         [-1, 64, 112, 112]             128\n         LeakyReLU-6         [-1, 64, 112, 112]               0\n            Conv2d-7         [-1, 32, 112, 112]           2,080\n       BatchNorm2d-8         [-1, 32, 112, 112]              64\n         LeakyReLU-9         [-1, 32, 112, 112]               0\n           Conv2d-10         [-1, 64, 112, 112]          18,496\n      BatchNorm2d-11         [-1, 64, 112, 112]             128\n        LeakyReLU-12         [-1, 64, 112, 112]               0\nDarkResidualBlock-13         [-1, 64, 112, 112]               0\n           Conv2d-14          [-1, 128, 56, 56]          73,856\n      BatchNorm2d-15          [-1, 128, 56, 56]             256\n        LeakyReLU-16          [-1, 128, 56, 56]               0\n           Conv2d-17           [-1, 64, 56, 56]           8,256\n      BatchNorm2d-18           [-1, 64, 56, 56]             128\n        LeakyReLU-19           [-1, 64, 56, 56]               0\n           Conv2d-20          [-1, 128, 56, 56]          73,856\n      BatchNorm2d-21          [-1, 128, 56, 56]             256\n        LeakyReLU-22          [-1, 128, 56, 56]               0\nDarkResidualBlock-23          [-1, 128, 56, 56]               0\n           Conv2d-24           [-1, 64, 56, 56]           8,256\n      BatchNorm2d-25           [-1, 64, 56, 56]             128\n        LeakyReLU-26           [-1, 64, 56, 56]               0\n           Conv2d-27          [-1, 128, 56, 56]          73,856\n      BatchNorm2d-28          [-1, 128, 56, 56]             256\n        LeakyReLU-29          [-1, 128, 56, 56]               0\nDarkResidualBlock-30          [-1, 128, 56, 56]               0\n           Conv2d-31          [-1, 256, 28, 28]         295,168\n      BatchNorm2d-32          [-1, 256, 28, 28]             512\n        LeakyReLU-33          [-1, 256, 28, 28]               0\n           Conv2d-34          [-1, 128, 28, 28]          32,896\n      BatchNorm2d-35          [-1, 128, 28, 28]             256\n        LeakyReLU-36          [-1, 128, 28, 28]               0\n           Conv2d-37          [-1, 256, 28, 28]         295,168\n      BatchNorm2d-38          [-1, 256, 28, 28]             512\n        LeakyReLU-39          [-1, 256, 28, 28]               0\nDarkResidualBlock-40          [-1, 256, 28, 28]               0\n           Conv2d-41          [-1, 128, 28, 28]          32,896\n      BatchNorm2d-42          [-1, 128, 28, 28]             256\n        LeakyReLU-43          [-1, 128, 28, 28]               0\n           Conv2d-44          [-1, 256, 28, 28]         295,168\n      BatchNorm2d-45          [-1, 256, 28, 28]             512\n        LeakyReLU-46          [-1, 256, 28, 28]               0\nDarkResidualBlock-47          [-1, 256, 28, 28]               0\n           Conv2d-48          [-1, 128, 28, 28]          32,896\n      BatchNorm2d-49          [-1, 128, 28, 28]             256\n        LeakyReLU-50          [-1, 128, 28, 28]               0\n           Conv2d-51          [-1, 256, 28, 28]         295,168\n      BatchNorm2d-52          [-1, 256, 28, 28]             512\n        LeakyReLU-53          [-1, 256, 28, 28]               0\nDarkResidualBlock-54          [-1, 256, 28, 28]               0\n           Conv2d-55          [-1, 128, 28, 28]          32,896\n      BatchNorm2d-56          [-1, 128, 28, 28]             256\n        LeakyReLU-57          [-1, 128, 28, 28]               0\n           Conv2d-58          [-1, 256, 28, 28]         295,168\n      BatchNorm2d-59          [-1, 256, 28, 28]             512\n        LeakyReLU-60          [-1, 256, 28, 28]               0\nDarkResidualBlock-61          [-1, 256, 28, 28]               0\n           Conv2d-62          [-1, 128, 28, 28]          32,896\n      BatchNorm2d-63          [-1, 128, 28, 28]             256\n        LeakyReLU-64          [-1, 128, 28, 28]               0\n           Conv2d-65          [-1, 256, 28, 28]         295,168\n      BatchNorm2d-66          [-1, 256, 28, 28]             512\n        LeakyReLU-67          [-1, 256, 28, 28]               0\nDarkResidualBlock-68          [-1, 256, 28, 28]               0\n           Conv2d-69          [-1, 128, 28, 28]          32,896\n      BatchNorm2d-70          [-1, 128, 28, 28]             256\n        LeakyReLU-71          [-1, 128, 28, 28]               0\n           Conv2d-72          [-1, 256, 28, 28]         295,168\n      BatchNorm2d-73          [-1, 256, 28, 28]             512\n        LeakyReLU-74          [-1, 256, 28, 28]               0\nDarkResidualBlock-75          [-1, 256, 28, 28]               0\n           Conv2d-76          [-1, 128, 28, 28]          32,896\n      BatchNorm2d-77          [-1, 128, 28, 28]             256\n        LeakyReLU-78          [-1, 128, 28, 28]               0\n           Conv2d-79          [-1, 256, 28, 28]         295,168\n      BatchNorm2d-80          [-1, 256, 28, 28]             512\n        LeakyReLU-81          [-1, 256, 28, 28]               0\nDarkResidualBlock-82          [-1, 256, 28, 28]               0\n           Conv2d-83          [-1, 128, 28, 28]          32,896\n      BatchNorm2d-84          [-1, 128, 28, 28]             256\n        LeakyReLU-85          [-1, 128, 28, 28]               0\n           Conv2d-86          [-1, 256, 28, 28]         295,168\n      BatchNorm2d-87          [-1, 256, 28, 28]             512\n        LeakyReLU-88          [-1, 256, 28, 28]               0\nDarkResidualBlock-89          [-1, 256, 28, 28]               0\n           Conv2d-90          [-1, 512, 14, 14]       1,180,160\n      BatchNorm2d-91          [-1, 512, 14, 14]           1,024\n        LeakyReLU-92          [-1, 512, 14, 14]               0\n           Conv2d-93          [-1, 256, 14, 14]         131,328\n      BatchNorm2d-94          [-1, 256, 14, 14]             512\n        LeakyReLU-95          [-1, 256, 14, 14]               0\n           Conv2d-96          [-1, 512, 14, 14]       1,180,160\n      BatchNorm2d-97          [-1, 512, 14, 14]           1,024\n        LeakyReLU-98          [-1, 512, 14, 14]               0\nDarkResidualBlock-99          [-1, 512, 14, 14]               0\n          Conv2d-100          [-1, 256, 14, 14]         131,328\n     BatchNorm2d-101          [-1, 256, 14, 14]             512\n       LeakyReLU-102          [-1, 256, 14, 14]               0\n          Conv2d-103          [-1, 512, 14, 14]       1,180,160\n     BatchNorm2d-104          [-1, 512, 14, 14]           1,024\n       LeakyReLU-105          [-1, 512, 14, 14]               0\nDarkResidualBlock-106          [-1, 512, 14, 14]               0\n          Conv2d-107          [-1, 256, 14, 14]         131,328\n     BatchNorm2d-108          [-1, 256, 14, 14]             512\n       LeakyReLU-109          [-1, 256, 14, 14]               0\n          Conv2d-110          [-1, 512, 14, 14]       1,180,160\n     BatchNorm2d-111          [-1, 512, 14, 14]           1,024\n       LeakyReLU-112          [-1, 512, 14, 14]               0\nDarkResidualBlock-113          [-1, 512, 14, 14]               0\n          Conv2d-114          [-1, 256, 14, 14]         131,328\n     BatchNorm2d-115          [-1, 256, 14, 14]             512\n       LeakyReLU-116          [-1, 256, 14, 14]               0\n          Conv2d-117          [-1, 512, 14, 14]       1,180,160\n     BatchNorm2d-118          [-1, 512, 14, 14]           1,024\n       LeakyReLU-119          [-1, 512, 14, 14]               0\nDarkResidualBlock-120          [-1, 512, 14, 14]               0\n          Conv2d-121          [-1, 256, 14, 14]         131,328\n     BatchNorm2d-122          [-1, 256, 14, 14]             512\n       LeakyReLU-123          [-1, 256, 14, 14]               0\n          Conv2d-124          [-1, 512, 14, 14]       1,180,160\n     BatchNorm2d-125          [-1, 512, 14, 14]           1,024\n       LeakyReLU-126          [-1, 512, 14, 14]               0\nDarkResidualBlock-127          [-1, 512, 14, 14]               0\n          Conv2d-128          [-1, 256, 14, 14]         131,328\n     BatchNorm2d-129          [-1, 256, 14, 14]             512\n       LeakyReLU-130          [-1, 256, 14, 14]               0\n          Conv2d-131          [-1, 512, 14, 14]       1,180,160\n     BatchNorm2d-132          [-1, 512, 14, 14]           1,024\n       LeakyReLU-133          [-1, 512, 14, 14]               0\nDarkResidualBlock-134          [-1, 512, 14, 14]               0\n          Conv2d-135          [-1, 256, 14, 14]         131,328\n     BatchNorm2d-136          [-1, 256, 14, 14]             512\n       LeakyReLU-137          [-1, 256, 14, 14]               0\n          Conv2d-138          [-1, 512, 14, 14]       1,180,160\n     BatchNorm2d-139          [-1, 512, 14, 14]           1,024\n       LeakyReLU-140          [-1, 512, 14, 14]               0\nDarkResidualBlock-141          [-1, 512, 14, 14]               0\n          Conv2d-142          [-1, 256, 14, 14]         131,328\n     BatchNorm2d-143          [-1, 256, 14, 14]             512\n       LeakyReLU-144          [-1, 256, 14, 14]               0\n          Conv2d-145          [-1, 512, 14, 14]       1,180,160\n     BatchNorm2d-146          [-1, 512, 14, 14]           1,024\n       LeakyReLU-147          [-1, 512, 14, 14]               0\nDarkResidualBlock-148          [-1, 512, 14, 14]               0\n          Conv2d-149           [-1, 1024, 7, 7]       4,719,616\n     BatchNorm2d-150           [-1, 1024, 7, 7]           2,048\n       LeakyReLU-151           [-1, 1024, 7, 7]               0\n          Conv2d-152            [-1, 512, 7, 7]         524,800\n     BatchNorm2d-153            [-1, 512, 7, 7]           1,024\n       LeakyReLU-154            [-1, 512, 7, 7]               0\n          Conv2d-155           [-1, 1024, 7, 7]       4,719,616\n     BatchNorm2d-156           [-1, 1024, 7, 7]           2,048\n       LeakyReLU-157           [-1, 1024, 7, 7]               0\nDarkResidualBlock-158           [-1, 1024, 7, 7]               0\n          Conv2d-159            [-1, 512, 7, 7]         524,800\n     BatchNorm2d-160            [-1, 512, 7, 7]           1,024\n       LeakyReLU-161            [-1, 512, 7, 7]               0\n          Conv2d-162           [-1, 1024, 7, 7]       4,719,616\n     BatchNorm2d-163           [-1, 1024, 7, 7]           2,048\n       LeakyReLU-164           [-1, 1024, 7, 7]               0\nDarkResidualBlock-165           [-1, 1024, 7, 7]               0\n          Conv2d-166            [-1, 512, 7, 7]         524,800\n     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n       LeakyReLU-168            [-1, 512, 7, 7]               0\n          Conv2d-169           [-1, 1024, 7, 7]       4,719,616\n     BatchNorm2d-170           [-1, 1024, 7, 7]           2,048\n       LeakyReLU-171           [-1, 1024, 7, 7]               0\nDarkResidualBlock-172           [-1, 1024, 7, 7]               0\n          Conv2d-173            [-1, 512, 7, 7]         524,800\n     BatchNorm2d-174            [-1, 512, 7, 7]           1,024\n       LeakyReLU-175            [-1, 512, 7, 7]               0\n          Conv2d-176           [-1, 1024, 7, 7]       4,719,616\n     BatchNorm2d-177           [-1, 1024, 7, 7]           2,048\n       LeakyReLU-178           [-1, 1024, 7, 7]               0\nDarkResidualBlock-179           [-1, 1024, 7, 7]               0\nAdaptiveAvgPool2d-180           [-1, 1024, 1, 1]               0\n          Linear-181                 [-1, 1000]       1,025,000\n================================================================\nTotal params: 41,627,784\nTrainable params: 41,627,784\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 249.23\nParams size (MB): 158.80\nEstimated Total Size (MB): 408.60\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "summary(model2, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "12"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "nn.Linear(122, 12).out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(623)"
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    " model(torch.FloatTensor(np.array([1] * 224 * 224 * 3).reshape(3, 224, 224)).unsqueeze(0)).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}